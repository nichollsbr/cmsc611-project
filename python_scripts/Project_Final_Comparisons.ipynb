{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to get JSON data from \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "#@MARIAMA - If you move spark-stats directory in all to the parent directory (what is base_dir below), then\n",
    "#you should be able to get rid of the following if statement.\n",
    "#Make sure you pull from master.  There is a new metrics directory in the parent directory, so do NOT pull\n",
    "#metrics from the all directory.\n",
    "\n",
    "stats_dir = \"spark-stats\"\n",
    "metrics_dir = \"metrics\"\n",
    "cluster_stats=\"cluster-spark-stats\"\n",
    "cluster_metrics=\"cluster_metrics\"\n",
    "base_dir = os.path.dirname(cwd)\n",
    "\n",
    "#provides an absolute path to spark-stats directory\n",
    "spark_stats_dir=\"\"\n",
    "#provides an absolute path to metrics directory (the run-output files)\n",
    "spark_metrics_dir=\"\"\n",
    "cluster_stats_dir=\"\"\n",
    "cluster_metrics_dir=\"\"\n",
    "if(cwd.startswith(\"C\")): #On a Windows\n",
    "    spark_stats_dir = base_dir + \"\\\\\" + stats_dir\n",
    "    spark_metrics_dir = base_dir + \"\\\\\" + metrics_dir\n",
    "    cluster_stats_dir = base_dir + \"\\\\\" + cluster_stats\n",
    "    cluster_metrics_dir = base_dir + \"\\\\\" + cluster_metrics\n",
    "else:\n",
    "    spark_stats_dir = base_dir + \"/\" + stats_dir\n",
    "    spark_metrics_dir = base_dir + \"/\" + metrics_dir\n",
    "    cluster_stats_dir = base_dir + \"/\" + cluster_stats\n",
    "    cluster_metrics_dir = base_dir + \"/\" + cluster_metrics\n",
    "\n",
    "metric_output_mapping = {\n",
    "    \"basic-dataframe-run-output\": \"BasicDataframe\",\n",
    "    \"basic-dataset-run-output\": \"BasicDataset\",\n",
    "    \"basic-rdd-run-output\": \"BasicRDD\",\n",
    "    \"cache-dataframe-run-output\": \"CacheDataframe\",\n",
    "    \"cache-dataset-run-output\": \"CacheDataset\",\n",
    "    \"cache-rdd-run-output\": \"CacheRDD\",\n",
    "    \"kmeans-dataframe-run-output\": \"KMeansDataframe\",\n",
    "    \"kmeans-dataset-run-output\": \"KMeansDataset\",\n",
    "    \"map-partitions-rdd-run-output\": \"MapPartitionsRDD\",\n",
    "    \"map-rdd-run-output\": \"MapRDD\",\n",
    "    \"partition-dataframe-run-output\": \"PartitionDataframe\",\n",
    "    \"parititon-dataset-run-output\": \"PartitionDataset\",\n",
    "    \"partition-rdd-run-output\": \"PartitionRDD\"\n",
    "}\n",
    "\n",
    "#Invert the above mapping\n",
    "metric_output_mapping_inv = {v: k for k, v in metric_output_mapping.items()}\n",
    "\n",
    "#Given spark-stats app name, get the file name\n",
    "def get_spark_stats_file(appName):\n",
    "    if(cwd.startswith(\"C\")):\n",
    "        return spark_stats_dir + \"\\\\\" + appName\n",
    "    else:\n",
    "        return spark_stats_dir + \"/\" + appName\n",
    "\n",
    "#Given the metrics output name, get the file name\n",
    "def get_metrics_file(fileName):\n",
    "    if(cwd.startswith(\"C\")):\n",
    "        return spark_metrics_dir + \"\\\\\" + fileName\n",
    "    else:\n",
    "        return spark_metrics_dir + \"/\" + fileName\n",
    "\n",
    "#Given a run-output file name (in metrics directory), get the appNames associated with that run\n",
    "def get_associated_appnames(run_output_name):\n",
    "    valid_files = list(pd.read_csv(get_metrics_file(run_output_name))[\"appName\"])\n",
    "    return [a_file for a_file in os.listdir(spark_stats_dir) if a_file in valid_files]\n",
    "\n",
    "#Given an appName (also a file name in the spark-stats directory), get the run parameters associated with that run\n",
    "def get_run_params(appName):\n",
    "    for beginningOutput in metric_output_mapping_inv:\n",
    "        if appName.startswith(beginningOutput):\n",
    "            metrics_df = pd.read_csv(get_metrics_file(metric_output_mapping_inv[beginningOutput]))\n",
    "            return metrics_df[metrics_df[\"appName\"] == appName]\n",
    "\n",
    "#Sort the metrics dataframe by threadCount, then executorMem, then overheadMem.  This will only apply to local jobs as is\n",
    "def metrics_df_sort(metrics_df):\n",
    "    return metrics_df.sort_values(by=['threadCount', 'executorMem', 'overheadMem'])\n",
    "\n",
    "#Get the metrics dataframe for the specified run-output file name. Then clean up and sort  This will only apply to local jobs as is\n",
    "def get_metrics(run_output_name):\n",
    "    raw_metrics = pd.read_csv(get_metrics_file(run_output_name))\n",
    "    raw_metrics[\"executorMem\"] = raw_metrics[\"executorMem\"].apply(lambda x: int(x[:-1]))\n",
    "    raw_metrics[\"overheadMem\"] = raw_metrics[\"overheadMem\"].apply(lambda x: int(x[:-1]))\n",
    "    return raw_metrics\n",
    "\n",
    "#Given an app name (file name in spark-stats since they match), get the dataframe for that app name\n",
    "def get_spark_stats(appName):\n",
    "    return pd.read_json(get_spark_stats_file(appName))\n",
    "\n",
    "print(cwd)\n",
    "print(base_dir)\n",
    "print(get_associated_appnames(\"basic-dataframe-run-output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just messing around. The output in this paragraph is a list of the stats available to be used\n",
    "map_partitions_files = get_associated_appnames(\"map-partitions-rdd-run-output\")\n",
    "map_files = get_associated_appnames(\"map-rdd-run-output\")\n",
    "print(map_partitions_files)\n",
    "print(map_files)\n",
    "sample = pd.read_json(get_spark_stats_file(map_partitions_files[0]))\n",
    "list(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"stageId\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the spark-stats dataframe, find the start time, end time, and run duration time in millis for the dataframe\n",
    "def get_full_execution_time(df):\n",
    "    start_time = min(df[\"launchTime\"])\n",
    "    end_time = max(df[\"finishTime\"])\n",
    "    time_range = end_time - start_time\n",
    "    return (start_time, end_time, time_range)\n",
    "\n",
    "# Given the spark-stats dataframe, calculate the total cpu time for the job\n",
    "def get_full_cpu_time(df):\n",
    "    return df[\"executorCpuTime\"].sum()\n",
    "\n",
    "# Given the spark-stats dataframe, calculate the max peakExecutionMemory for the job\n",
    "def get_max_peakExecutionMemory(df):\n",
    "    if df is None:\n",
    "        return 0\n",
    "    return max(df[\"peakExecutionMemory\"])\n",
    "\n",
    "# Given the spark-stats dataframe, calculate the min peakExecutionMemory for the job\n",
    "def get_min_peakExecutionMemory(df):\n",
    "    return min(df[\"peakExecutionMemory\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = get_full_execution_time(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_run_params(map_partitions_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(data, xlabel, ylabel, title, savefig=\"graph.png\"):\n",
    "    ax = sns.lineplot(data=data, linewidth=2.5)\n",
    "    ax.set(xlabel=xlabel, ylabel=ylabel, title=title)\n",
    "    plt.savefig(\"images/\"+ savefig)\n",
    "\n",
    "    #Cleanup the plot that we just printed so the one below doesn't have extra data\n",
    "    plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Partitions RDD vs Map RDD\n",
    "# Line plot of the job num vs execution time\n",
    "# Note that basic-rdd and map-partitions are the exact same.  They were just labelled differently.\n",
    "# I was curious to see if they exhibited the same run times, but they don't.  I don't know how to explain the differences\n",
    "mapPartitions_vs_map_df = {\n",
    "    \"BasicRDD\": get_metrics(\"basic-rdd-run-output\")[\"appName\"].apply(lambda appName:get_full_execution_time(get_spark_stats(appName))[2]/1000.0),\n",
    "    \"MapPartitions\": get_metrics(\"map-partitions-rdd-run-output\")[\"appName\"].apply(lambda appName:get_full_execution_time(get_spark_stats(appName))[2]/1000.0),\n",
    "    \"Map\": get_metrics(\"map-rdd-run-output\")[\"appName\"].apply(lambda appName:get_full_execution_time(get_spark_stats(appName))[2]/1000.0),  \n",
    "}\n",
    "data = pd.DataFrame(mapPartitions_vs_map_df)\n",
    "\n",
    "#Create a line plot for all 3 jobs.  This is what shows there are weird anomalies in the execution time for the \n",
    "# mapPartitions/basicRDD jobs\n",
    "generate_graph(data, \"Job Number\", \"Total Execution Time (sec)\", title=\"Execution Time Comparison When\\nUsing Map vs MapPartitions vs BasicRDD\", savefig=\"mapVsMapPartitionsAllExecutionTime.png\")\n",
    "\n",
    "#Cleanup the plot that we just printed so the one below doesn't have extra data\n",
    "plt.cla()\n",
    "\n",
    "#Now do cleanup. Since BasicRDD and MapParitions are the same job, let's just take the minimum execution time\n",
    "#to compare to the execution time of the map job.\n",
    "data[\"MapPartitions\"] = data[[\"MapPartitions\", \"BasicRDD\"]].min(axis=1)\n",
    "data = data.drop([\"BasicRDD\"], axis=1)\n",
    "\n",
    "generate_graph(data, \"Job Number\", \"Total Execution Time (sec)\", title=\"Execution Time Comparison When Using Map vs MapPartitions\", savefig=\"mapVsMapPartitionsBestExecutionTime.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wanted to see what comparing the CPU Time would look like.\n",
    "mapPartitions_vs_map_df = {\n",
    "    \"BasicRDD\": get_metrics(\"basic-rdd-run-output\")[\"appName\"].apply(lambda appName:get_full_cpu_time(get_spark_stats(appName))),\n",
    "    \"MapPartitions\": get_metrics(\"map-partitions-rdd-run-output\")[\"appName\"].apply(lambda appName:get_full_cpu_time(get_spark_stats(appName))),\n",
    "    \"Map\": get_metrics(\"map-rdd-run-output\")[\"appName\"].apply(lambda appName:get_full_cpu_time(get_spark_stats(appName))),  \n",
    "}\n",
    "data = pd.DataFrame(mapPartitions_vs_map_df)\n",
    "\n",
    "#Again, just seeing how BasicRDD and MapPartitions jobs compare.\n",
    "generate_graph(data, \"Job Number\", \"Total CPU Time of Executors (sec)\", \n",
    "               title=\"CPU Execution Time Comparison When\\nUsing Map vs MapPartitions vs BasicRDD\", savefig=\"mapVsMapPartitionsAllCpuTime.png\")\n",
    "\n",
    "#Now, just seeing how minimum CPU Time of BasicRDD and MapPartitions jobs compare to the Map job.\n",
    "data[\"MapPartitions\"] = data[[\"MapPartitions\", \"BasicRDD\"]].min(axis=1)\n",
    "data = data.drop([\"BasicRDD\"], axis=1)\n",
    "\n",
    "generate_graph(data, \"Job Number\", \"Total CPU Time of Executors (sec)\", \n",
    "               title=\"CPU Execution Time Comparison When Using Map vs MapPartitions\", savefig=\"mapVsMapPartitionsBestCpuTime.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I'm comparing the max peak execution memory of the mapParititons and the map jobs.\n",
    "mapPartitions_vs_map_df = {\n",
    "    \"MapPartitions\": get_metrics(\"map-partitions-rdd-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_spark_stats(appName))),\n",
    "    \"Map\": get_metrics(\"map-rdd-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_spark_stats(appName))),  \n",
    "}\n",
    "data = pd.DataFrame(mapPartitions_vs_map_df)\n",
    "\n",
    "generate_graph(data, \"Job Number\", \"Max Peak Execution Memory (bytes)\", \n",
    "               title=\"Max Peak Execution Memory Comparison When\\nUsing Map vs MapPartitions\", savefig=\"mapVsMapPartitionsMaxPeakExecutionMemory.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I'm comparing the min peak execution memory of the mapParititons and the map jobs.  Was curious to see\n",
    "mapPartitions_vs_map_df = {\n",
    "    \"MapPartitions\": get_metrics(\"map-partitions-rdd-run-output\")[\"appName\"].apply(lambda appName:get_min_peakExecutionMemory(get_spark_stats(appName))),\n",
    "    \"Map\": get_metrics(\"map-rdd-run-output\")[\"appName\"].apply(lambda appName:get_min_peakExecutionMemory(get_spark_stats(appName))),  \n",
    "}\n",
    "data = pd.DataFrame(mapPartitions_vs_map_df)\n",
    "\n",
    "generate_graph(data, \"Job Number\", \"Min Peak Execution Memory (bytes)\", \n",
    "               title=\"Min Peak Execution Memory Comparison When\\nUsing Map vs MapPartitions\", savefig=\"mapVsMapPartitionsMinPeakExecutionMemory.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's get metrics on the input of the job\n",
    "sample_mapPartitionsJob_appName = get_metrics(\"map-partitions-rdd-run-output\")[\"appName\"][0]\n",
    "df = get_spark_stats(sample_mapPartitionsJob_appName)\n",
    "sum(df[df[\"stageId\"] == 0][\"bytesRead\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df[df[\"stageId\"] == 0][\"recordsRead\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_jobs_df = {\n",
    "    \"BasicRDD\": get_metrics(\"basic-rdd-run-output\")[\"appName\"].apply(lambda appName:get_full_execution_time(get_spark_stats(appName))[2]/1000.0),\n",
    "    \"BasicDataset\": get_metrics(\"basic-dataset-run-output\")[\"appName\"].apply(lambda appName:get_full_execution_time(get_spark_stats(appName))[2]/1000.0),\n",
    "    \"BasicDataframe\": get_metrics(\"basic-dataframe-run-output\")[\"appName\"].apply(lambda appName:get_full_execution_time(get_spark_stats(appName))[2]/1000.0),  \n",
    "}\n",
    "data = pd.DataFrame(basic_jobs_df)\n",
    "\n",
    "#Create a line plot for all 3 jobs.  This is what shows there are weird anomalies in the execution time for the \n",
    "# mapPartitions/basicRDD jobs\n",
    "generate_graph(data, \"Job Number\", \"Total Execution Time (sec)\", title=\"Execution Time Comparison When Running\\nBasic Count Job for RDD vs Dataset vs DataFrame\", savefig=\"basicJobAllExecutionTime.png\")\n",
    "\n",
    "basic_jobs_df = {\n",
    "    \"BasicRDD\": get_metrics(\"basic-rdd-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_spark_stats(appName))),\n",
    "    \"BasicDataset\": get_metrics(\"basic-dataset-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_spark_stats(appName))),  \n",
    "    \"BasicDataframe\": get_metrics(\"basic-dataframe-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_spark_stats(appName))),  \n",
    "}\n",
    "data = pd.DataFrame(basic_jobs_df)\n",
    "\n",
    "generate_graph(data, \"Job Number\", \"Max Peak Execution Memory (bytes)\", \n",
    "               title=\"Max Peak Execution Memory Comparison Running\\nBasic Count Job for RDD vs Dataset vs DataFrame\", savefig=\"basicJobMaxPeakExecutionMemory.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_jobs_df = {\n",
    "    \"PartitionRDD\": get_metrics(\"partition-rdd-run-output\")[\"appName\"].apply(lambda appName:get_full_execution_time(get_spark_stats(appName))[2]/1000.0),\n",
    "    \"PartitionDataset\": get_metrics(\"partition-dataset-run-output\")[\"appName\"].apply(lambda appName:get_full_execution_time(get_spark_stats(appName))[2]/1000.0),\n",
    "    \"PartitionDataframe\": get_metrics(\"partition-dataframe-run-output\")[\"appName\"].apply(lambda appName:get_full_execution_time(get_spark_stats(appName))[2]/1000.0),  \n",
    "}\n",
    "data = pd.DataFrame(partition_jobs_df)\n",
    "\n",
    "#Create a line plot for all 3 jobs.  This is what shows there are weird anomalies in the execution time for the \n",
    "# mapPartitions/basicRDD jobs\n",
    "generate_graph(data, \"Job Number\", \"Total Execution Time (sec)\", title=\"Execution Time Comparison When Running\\nPartition Job for RDD vs Dataset vs DataFrame\", savefig=\"partitionJobAllExecutionTime.png\")\n",
    "\n",
    "partition_jobs_df = {\n",
    "    \"PartitionRDD\": get_metrics(\"partition-rdd-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_spark_stats(appName))),\n",
    "    \"PartitionDataset\": get_metrics(\"partition-dataset-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_spark_stats(appName))),  \n",
    "    \"PartitionDataframe\": get_metrics(\"partition-dataframe-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_spark_stats(appName))),  \n",
    "}\n",
    "data = pd.DataFrame(partition_jobs_df)\n",
    "\n",
    "generate_graph(data, \"Job Number\", \"Max Peak Execution Memory (bytes)\", \n",
    "               title=\"Max Peak Execution Memory Comparison Running\\nPartition Job for RDD vs Dataset vs DataFrame\", savefig=\"partitionJobMaxPeakExecutionMemory.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_jobs_df = {\n",
    "    \"CacheRDD\": get_metrics(\"cache-rdd-run-output\")[\"appName\"].apply(lambda appName:get_full_execution_time(get_spark_stats(appName))[2]/1000.0),\n",
    "    \"CacheDataset\": get_metrics(\"cache-dataset-run-output\")[\"appName\"].apply(lambda appName:get_full_execution_time(get_spark_stats(appName))[2]/1000.0),\n",
    "    \"CacheDataframe\": get_metrics(\"cache-dataframe-run-output\")[\"appName\"].apply(lambda appName:get_full_execution_time(get_spark_stats(appName))[2]/1000.0),  \n",
    "}\n",
    "data = pd.DataFrame(cache_jobs_df)\n",
    "\n",
    "#Create a line plot for all 3 jobs.  This is what shows there are weird anomalies in the execution time for the \n",
    "# mapPartitions/basicRDD jobs\n",
    "generate_graph(data, \"Job Number\", \"Total Execution Time (sec)\", title=\"Execution Time Comparison When Running\\nCache Job for RDD vs Dataset vs DataFrame\", savefig=\"cacheJobAllExecutionTime.png\")\n",
    "\n",
    "cache_jobs_df = {\n",
    "    \"CacheRDD\": get_metrics(\"cache-rdd-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_spark_stats(appName))),\n",
    "    \"CacheDataset\": get_metrics(\"cache-dataset-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_spark_stats(appName))),  \n",
    "    \"CacheDataframe\": get_metrics(\"cache-dataframe-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_spark_stats(appName))),  \n",
    "}\n",
    "data = pd.DataFrame(cache_jobs_df)\n",
    "\n",
    "generate_graph(data, \"Job Number\", \"Max Peak Execution Memory (bytes)\", \n",
    "               title=\"Max Peak Execution Memory Comparison Running\\nCache Job for RDD vs Dataset vs DataFrame\", savefig=\"cacheJobMaxPeakExecutionMemory.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_jobs_df = {\n",
    "#     \"KMeansRDD\": get_metrics(\"kmeans-rdd-run-output\")[\"appName\"].apply(lambda appName:get_full_execution_time(get_spark_stats(appName))[2]/1000.0),\n",
    "    \"KMeansDataset\": get_metrics(\"kmeans-dataset-run-output\")[\"appName\"].apply(lambda appName:get_full_execution_time(get_spark_stats(appName))[2]/1000.0),\n",
    "    \"KMeansDataframe\": get_metrics(\"kmeans-dataframe-run-output\")[\"appName\"].apply(lambda appName:get_full_execution_time(get_spark_stats(appName))[2]/1000.0),  \n",
    "}\n",
    "data = pd.DataFrame(kmeans_jobs_df)\n",
    "\n",
    "#Create a line plot for all 3 jobs.  This is what shows there are weird anomalies in the execution time for the \n",
    "# mapPartitions/basicRDD jobs\n",
    "generate_graph(data, \"Job Number\", \"Total Execution Time (sec)\", title=\"Execution Time Comparison When Running\\nKMeans Job for RDD vs Dataset vs DataFrame\", savefig=\"kmeansJobAllExecutionTime.png\")\n",
    "\n",
    "kmeans_jobs_df = {\n",
    "#     \"KMeansRDD\": get_metrics(\"kmeans-rdd-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_spark_stats(appName))),\n",
    "    \"KMeansDataset\": get_metrics(\"kmeans-dataset-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_spark_stats(appName))),  \n",
    "    \"KMeansDataframe\": get_metrics(\"kmeans-dataframe-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_spark_stats(appName))),  \n",
    "}\n",
    "data = pd.DataFrame(kmeans_jobs_df)\n",
    "\n",
    "generate_graph(data, \"Job Number\", \"Max Peak Execution Memory (bytes)\", \n",
    "               title=\"Max Peak Execution Memory Comparison Running\\nKMeans Job for RDD vs Dataset vs DataFrame\", savefig=\"kmeansJobMaxPeakExecutionMemory.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxPeakExecutionMemory = 0\n",
    "for appName in get_metrics(\"map-partitions-rdd-run-output\")[\"appName\"]:\n",
    "    maxPeakExecutionMemory = max(maxPeakExecutionMemory, get_max_peakExecutionMemory(get_spark_stats(appName)))\n",
    "sample_df = get_spark_stats(get_metrics(\"map-partitions-rdd-run-output\")[\"appName\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[sample_df[\"peakExecutionMemory\"] == maxPeakExecutionMemory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Okay, this is prep for the two graphics below.  Basically, take all of the stats for the map Partitions jobs\n",
    "#and union them together.\n",
    "apps = get_metrics(\"map-partitions-rdd-run-output\")[\"appName\"]\n",
    "df = get_spark_stats(apps[0])\n",
    "for appName in apps[1:]:\n",
    "    df = df.append(get_spark_stats(appName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wanted to see if the peak execution memory related to the bytes input/output.  This is running a linear regression\n",
    "#(a best fit line) comparing the peakExecutionMemory for a task and the bytes read into the task.\n",
    "# It looks like they are somewhat related, but I wouldn't say they are strongly related.\n",
    "ax = sns.lmplot(x=\"peakExecutionMemory\", y=\"bytesRead\", truncate=True, data=df)\n",
    "ax.set(xlabel='Peak Execution Memory (bytes)', \n",
    "       xlim = (0,max(df[\"peakExecutionMemory\"])),\n",
    "       ylabel='Bytes Read (bytes)', \n",
    "       ylim = (0,max(df[\"bytesRead\"])),\n",
    "       title=\"Peak Execution Memory vs Bytes Read for MapPartitions RDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is interesting.  It really looks like the peakExecutionMemory and the shuffleBytesWritten (the output of a task)\n",
    "# are strongly related.  I'd like to see if this pattern continues through the other comparisons. Might be a good\n",
    "# talking point.\n",
    "ax = sns.lmplot(x=\"peakExecutionMemory\", y=\"shuffleBytesWritten\", truncate=True, data=df)\n",
    "ax.set(xlabel='Peak Execution Memory (bytes)', \n",
    "       xlim = (0,max(df[\"peakExecutionMemory\"])),\n",
    "       ylabel='Shuffle Bytes Written (bytes)', \n",
    "       ylim = (0,max(df[\"shuffleBytesWritten\"])),\n",
    "       title=\"Peak Execution Memory vs Bytes Read for MapPartitions RDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_metric_output_mapping = {\n",
    "    \"basic-dataframe-cluster-run-output\": \"BasicDataframe\",\n",
    "    \"basic-dataset-cluster-run-output\": \"BasicDataset\",\n",
    "    \"basic-rdd-cluster-run-output\": \"BasicRDD\",\n",
    "    \"cache-dataframe-cluster-run-output\": \"CacheDataframe\",\n",
    "    \"cache-dataset-cluster-run-output\": \"CacheDataset\",\n",
    "    \"cache-rdd-cluster-run-output\": \"CacheRDD\",\n",
    "    \"kmeans-dataframe-cluster-run-output\": \"KMeansDataframe\",\n",
    "    \"kmeans-dataset-cluster-run-output\": \"KMeansDataset\",\n",
    "    \"map-partitions-cluster-rdd-run-output\": \"MapPartitionsRDD\",\n",
    "    \"map-rdd-cluster-run-output\": \"MapRDD\",\n",
    "    \"partition-dataframe-cluster-run-output\": \"PartitionDataframe\",\n",
    "    \"parititon-dataset-cluster-run-output\": \"PartitionDataset\",\n",
    "    \"partition-rdd-cluster-run-output\": \"PartitionRDD\"\n",
    "}\n",
    "\n",
    "#Invert the above mapping\n",
    "cluster_metric_output_mapping_inv = {v: k for k, v in cluster_metric_output_mapping.items()}\n",
    "\n",
    "#Given spark-stats app name, get the file name\n",
    "def get_cluster_spark_stats_file(appName):\n",
    "    if(cwd.startswith(\"C\")):\n",
    "        return cluster_stats_dir + \"\\\\\" + appName\n",
    "    else:\n",
    "        return cluster_stats_dir + \"/\" + appName\n",
    "\n",
    "#Given the metrics output name, get the file name\n",
    "def get_cluster_metrics_file(fileName):\n",
    "    if(cwd.startswith(\"C\")):\n",
    "        return cluster_metrics_dir + \"\\\\\" + fileName\n",
    "    else:\n",
    "        return cluster_metrics_dir + \"/\" + fileName\n",
    "\n",
    "#Given a run-output file name (in metrics directory), get the appNames associated with that run\n",
    "def get_cluster_associated_appnames(run_output_name):\n",
    "    valid_files = list(pd.read_csv(get_cluster_metrics_file(run_output_name))[\"appName\"])\n",
    "    return [a_file for a_file in os.listdir(cluster_stats_dir) if a_file in valid_files]\n",
    "\n",
    "#Given an appName (also a file name in the spark-stats directory), get the run parameters associated with that run\n",
    "def get_cluster_run_params(appName):\n",
    "    for beginningOutput in cluster_metric_output_mapping_inv:\n",
    "        if appName.startswith(beginningOutput):\n",
    "            metrics_df = pd.read_csv(get_cluster_metrics_file(cluster_metric_output_mapping_inv[beginningOutput]))\n",
    "            return metrics_df[metrics_df[\"appName\"] == appName]\n",
    "\n",
    "#Given an app name (file name in spark-stats since they match), get the dataframe for that app name\n",
    "def get_cluster_spark_stats(appName):\n",
    "    filetoUse = get_cluster_spark_stats_file(appName)\n",
    "    try:\n",
    "        return pd.read_json(filetoUse)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "#Get the metrics dataframe for the specified run-output file name. Then clean up and sort  This will only apply to local jobs as is\n",
    "def get_cluster_metrics(run_output_name):\n",
    "    raw_metrics = pd.read_csv(get_cluster_metrics_file(run_output_name))\n",
    "    raw_metrics[\"executorMem\"] = raw_metrics[\"executorMem\"].apply(lambda x: int(x[:-1]))\n",
    "    raw_metrics[\"overheadMem\"] = raw_metrics[\"overheadMem\"].apply(lambda x: int(x[:-1]))\n",
    "    return raw_metrics\n",
    "\n",
    "def get_cluster_full_execution_time(df):\n",
    "    if df is None:\n",
    "        return (0, 0, -1000)\n",
    "    start_time = min(df[\"submissionTime\"])\n",
    "    end_time = max(df[\"completionTime\"])\n",
    "    time_range = end_time - start_time\n",
    "    return (start_time, end_time, time_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Partitions RDD vs Map RDD\n",
    "# Line plot of the job num vs execution time\n",
    "# Note that basic-rdd and map-partitions are the exact same.  They were just labelled differently.\n",
    "# I was curious to see if they exhibited the same run times, but they don't.  I don't know how to explain the differences\n",
    "mapPartitions_vs_map_df = {\n",
    "    \"BasicRDD\": get_cluster_metrics(\"basic-rdd-cluster-run-output\")[\"appName\"].apply(lambda appName:get_cluster_full_execution_time(get_cluster_spark_stats(appName))[2]/1000.0),\n",
    "    \"MapPartitions\": get_cluster_metrics(\"map-partitions-rdd-cluster-run-output\")[\"appName\"].apply(lambda appName:get_cluster_full_execution_time(get_cluster_spark_stats(appName))[2]/1000.0),\n",
    "    \"Map\": get_cluster_metrics(\"map-rdd-cluster-run-output\")[\"appName\"].apply(lambda appName:get_cluster_full_execution_time(get_cluster_spark_stats(appName))[2]/1000.0),  \n",
    "}\n",
    "data = pd.DataFrame(mapPartitions_vs_map_df)\n",
    "\n",
    "#Create a line plot for all 3 jobs.  This is what shows there are weird anomalies in the execution time for the \n",
    "# mapPartitions/basicRDD jobs\n",
    "generate_graph(data, \"Job Number\", \"Total Execution Time (sec)\", title=\"Execution Time Comparison When\\nUsing Map vs MapPartitions vs BasicRDD On Cluster\", savefig=\"mapVsMapPartitionsAllExecutionTimeCluster.png\")\n",
    "\n",
    "#Cleanup the plot that we just printed so the one below doesn't have extra data\n",
    "# plt.cla()\n",
    "\n",
    "#Now do cleanup. Since BasicRDD and MapParitions are the same job, let's just take the minimum execution time\n",
    "#to compare to the execution time of the map job.\n",
    "data[\"MapPartitions\"] = data[[\"MapPartitions\", \"BasicRDD\"]].min(axis=1)\n",
    "data = data.drop([\"BasicRDD\"], axis=1)\n",
    "\n",
    "generate_graph(data, \"Job Number\", \"Total Execution Time (sec)\", title=\"Execution Time Comparison When Using Map vs MapPartitions\", savefig=\"mapVsMapPartitionsBestExecutionTimeCluster.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I'm comparing the max peak execution memory of the mapParititons and the map jobs.\n",
    "mapPartitions_vs_map_df = {\n",
    "    \"MapPartitions\": get_cluster_metrics(\"map-partitions-rdd-cluster-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_cluster_spark_stats(appName))),\n",
    "    \"Map\": get_cluster_metrics(\"map-rdd-cluster-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_cluster_spark_stats(appName))),  \n",
    "#     \"BasicRDD\": get_cluster_metrics(\"basic-rdd-cluster-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_cluster_spark_stats(appName))),  \n",
    "}\n",
    "data = pd.DataFrame(mapPartitions_vs_map_df)\n",
    "\n",
    "generate_graph(data, \"Job Number\", \"Max Peak Execution Memory (bytes)\", \n",
    "               title=\"Max Peak Execution Memory Comparison When\\nUsing Map vs MapPartitions on Cluster\", savefig=\"mapVsMapPartitionsMaxPeakExecutionMemoryCluster.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cluster_metrics(\"basic-rdd-cluster-run-output\")[\"appName\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's get metrics on the input of the job\n",
    "sample_mapPartitionsJob_appName = get_cluster_metrics(\"map-partitions-rdd-cluster-run-output\")[\"appName\"][0]\n",
    "df = get_cluster_spark_stats(sample_mapPartitionsJob_appName)\n",
    "sum(df[df[\"stageId\"] == 0][\"bytesRead\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df[df[\"stageId\"] == 0][\"recordsRead\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_jobs_df = {\n",
    "    \"BasicRDD\": get_cluster_metrics(\"basic-rdd-cluster-run-output\")[\"appName\"].apply(lambda appName:get_cluster_full_execution_time(get_cluster_spark_stats(appName))[2]/1000.0),\n",
    "    \"BasicDataset\": get_cluster_metrics(\"basic-dataset-cluster-run-output\")[\"appName\"].apply(lambda appName:get_cluster_full_execution_time(get_cluster_spark_stats(appName))[2]/1000.0),\n",
    "    \"BasicDataFrame\": get_cluster_metrics(\"basic-dataframe-cluster-run-output\")[\"appName\"].apply(lambda appName:get_cluster_full_execution_time(get_cluster_spark_stats(appName))[2]/1000.0),  \n",
    "}\n",
    "data = pd.DataFrame(basic_jobs_df)\n",
    "\n",
    "#Create a line plot for all 3 jobs.  This is what shows there are weird anomalies in the execution time for the \n",
    "# mapPartitions/basicRDD jobs\n",
    "generate_graph(data, \"Job Number\", \"Total Execution Time (sec)\", title=\"Execution Time Comparison When Using Count Jobs of\\nRDD vs DataFrame vs Dataset On Cluster\", savefig=\"basicJobsAllExecutionTimeCluster.png\")\n",
    "\n",
    "basic_jobs_df = {\n",
    "    \"BasicDataset\": get_cluster_metrics(\"basic-dataset-cluster-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_cluster_spark_stats(appName))),\n",
    "    \"BasicDataFrame\": get_cluster_metrics(\"basic-dataframe-cluster-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_cluster_spark_stats(appName))),  \n",
    "    \"BasicRDD\": get_cluster_metrics(\"basic-rdd-cluster-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_cluster_spark_stats(appName))),  \n",
    "}\n",
    "data = pd.DataFrame(basic_jobs_df)\n",
    "\n",
    "generate_graph(data, \"Job Number\", \"Max Peak Execution Memory (bytes)\", \n",
    "               title=\"Max Peak Execution Memory Comparison When Using Count Jobs of\\nRDD vs DataFrame vs Dataset On Cluster\", savefig=\"basicJobsMaxPeakExecutionMemoryCluster.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_jobs_df = {\n",
    "    \"PartitionRDD\": get_cluster_metrics(\"partition-rdd-cluster-run-output\")[\"appName\"].apply(lambda appName:get_cluster_full_execution_time(get_cluster_spark_stats(appName))[2]/1000.0),\n",
    "    \"PartitionDataset\": get_cluster_metrics(\"partition-dataset-cluster-run-output\")[\"appName\"].apply(lambda appName:get_cluster_full_execution_time(get_cluster_spark_stats(appName))[2]/1000.0),\n",
    "    \"PartitionDataFrame\": get_cluster_metrics(\"partition-dataframe-cluster-run-output\")[\"appName\"].apply(lambda appName:get_cluster_full_execution_time(get_cluster_spark_stats(appName))[2]/1000.0),  \n",
    "}\n",
    "data = pd.DataFrame(partition_jobs_df)\n",
    "\n",
    "#Create a line plot for all 3 jobs.  This is what shows there are weird anomalies in the execution time for the \n",
    "# mapPartitions/basicRDD jobs\n",
    "generate_graph(data, \"Job Number\", \"Total Execution Time (sec)\", title=\"Execution Time Comparison When Repartitioning\\nRDD vs DataFrame vs Dataset On Cluster\", savefig=\"partitionJobsAllExecutionTimeCluster.png\")\n",
    "\n",
    "partition_jobs_df = {\n",
    "    \"PartitionDataset\": get_cluster_metrics(\"partition-dataset-cluster-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_cluster_spark_stats(appName))),\n",
    "    \"PartitionDataFrame\": get_cluster_metrics(\"partition-dataframe-cluster-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_cluster_spark_stats(appName))),  \n",
    "    \"PartitionRDD\": get_cluster_metrics(\"partition-rdd-cluster-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_cluster_spark_stats(appName))),  \n",
    "}\n",
    "data = pd.DataFrame(partition_jobs_df)\n",
    "\n",
    "generate_graph(data, \"Job Number\", \"Max Peak Execution Memory (bytes)\", \n",
    "               title=\"Max Peak Execution Memory Comparison When Repartitioning\\nRDD vs DataFrame vs Dataset On Cluster\", savefig=\"partitionJobsMaxPeakExecutionMemoryCluster.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_jobs_df = {\n",
    "    \"PartitionRDD\": get_cluster_metrics(\"partition-rdd-cluster-run-output\")[\"appName\"].apply(lambda appName:get_cluster_full_execution_time(get_cluster_spark_stats(appName))[2]/1000.0),\n",
    "    \"PartitionDataset\": get_cluster_metrics(\"partition-dataset-cluster-run-output\")[\"appName\"].apply(lambda appName:get_cluster_full_execution_time(get_cluster_spark_stats(appName))[2]/1000.0),\n",
    "    \"PartitionDataFrame\": get_cluster_metrics(\"partition-dataframe-cluster-run-output\")[\"appName\"].apply(lambda appName:get_cluster_full_execution_time(get_cluster_spark_stats(appName))[2]/1000.0),  \n",
    "}\n",
    "data = pd.DataFrame(partition_jobs_df)\n",
    "\n",
    "#Create a line plot for all 3 jobs.  This is what shows there are weird anomalies in the execution time for the \n",
    "# mapPartitions/basicRDD jobs\n",
    "generate_graph(data, \"Job Number\", \"Total Execution Time (sec)\", title=\"Execution Time Comparison When Repartitioning\\nRDD vs DataFrame vs Dataset On Cluster\", savefig=\"partitionJobsAllExecutionTimeCluster.png\")\n",
    "\n",
    "partition_jobs_df = {\n",
    "    \"PartitionDataset\": get_cluster_metrics(\"partition-dataset-cluster-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_cluster_spark_stats(appName))),\n",
    "    \"PartitionDataFrame\": get_cluster_metrics(\"partition-dataframe-cluster-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_cluster_spark_stats(appName))),  \n",
    "    \"PartitionRDD\": get_cluster_metrics(\"partition-rdd-cluster-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_cluster_spark_stats(appName))),  \n",
    "}\n",
    "data = pd.DataFrame(partition_jobs_df)\n",
    "\n",
    "generate_graph(data, \"Job Number\", \"Max Peak Execution Memory (bytes)\", \n",
    "               title=\"Max Peak Execution Memory Comparison When Repartitioning\\nRDD vs DataFrame vs Dataset On Cluster\", savefig=\"partitionJobsMaxPeakExecutionMemoryCluster.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_jobs_df = {\n",
    "    \"CacheRDD\": get_cluster_metrics(\"cache-rdd-cluster-run-output\")[\"appName\"].apply(lambda appName:get_cluster_full_execution_time(get_cluster_spark_stats(appName))[2]/1000.0),\n",
    "    \"CacheDataset\": get_cluster_metrics(\"cache-dataset-cluster-run-output\")[\"appName\"].apply(lambda appName:get_cluster_full_execution_time(get_cluster_spark_stats(appName))[2]/1000.0),\n",
    "    \"CacheDataFrame\": get_cluster_metrics(\"cache-dataframe-cluster-run-output\")[\"appName\"].apply(lambda appName:get_cluster_full_execution_time(get_cluster_spark_stats(appName))[2]/1000.0),  \n",
    "}\n",
    "data = pd.DataFrame(cache_jobs_df)\n",
    "\n",
    "#Create a line plot for all 3 jobs.  This is what shows there are weird anomalies in the execution time for the \n",
    "# mapPartitions/basicRDD jobs\n",
    "generate_graph(data, \"Job Number\", \"Total Execution Time (sec)\", title=\"Execution Time Comparison When Caching\\nRDD vs DataFrame vs Dataset On Cluster\", savefig=\"cacheJobsAllExecutionTimeCluster.png\")\n",
    "\n",
    "cache_jobs_df = {\n",
    "    \"CacheDataset\": get_cluster_metrics(\"cache-dataset-cluster-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_cluster_spark_stats(appName))),\n",
    "    \"CacheDataFrame\": get_cluster_metrics(\"cache-dataframe-cluster-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_cluster_spark_stats(appName))),  \n",
    "    \"CacheRDD\": get_cluster_metrics(\"cache-rdd-cluster-run-output\")[\"appName\"].apply(lambda appName:get_max_peakExecutionMemory(get_cluster_spark_stats(appName))),  \n",
    "}\n",
    "data = pd.DataFrame(cache_jobs_df)\n",
    "\n",
    "generate_graph(data, \"Job Number\", \"Max Peak Execution Memory (bytes)\", \n",
    "               title=\"Max Peak Execution Memory Comparison When Caching\\nRDD vs DataFrame vs Dataset On Cluster\", savefig=\"cacheJobsMaxPeakExecutionMemoryCluster.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
