\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\graphicspath{  {./images/}  }
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Benchmarking Resource Usage of Underlying Datatypes of Apache Spark\\
}

\author{\IEEEauthorblockN{Brittany Nicholls}
\IEEEauthorblockA{\textit{CSEE} \\
\textit{UMBC}\\
Catonsville, United States \\
brn1@umbc.edu}
\and
\IEEEauthorblockN{Mariama Adangwa}
\IEEEauthorblockA{\textit{CSEE} \\
\textit{University of Maryland, Baltimore County (UMBC)}\\
Catonsville, United States \\
bama4@umbc.edu}
\and
\IEEEauthorblockN{Rachel Estes}
\IEEEauthorblockA{\textit{CSEE} \\
\textit{UMBC}\\
Catonsville, United States \\
email address}
}

\maketitle

\begin{abstract}
The purpose of this paper is examine how the different underlying datatypes of Spark jobs - Resilient Distributed Datasets (RDDs),
Datasets, and DataFrames - affect resource usage.
A case is made against using execution time as the primary benchmark of analytics since many external factors can affect the execution time.
Instead, metrics readily available through Spark are used to benchmark the resource usage of these different datatypes in common applications of Spark jobs, such as counting, caching, repartitioning and K-Means.
\end{abstract}

\begin{IEEEkeywords}
apache,spark,benchmark,analytics,big data
\end{IEEEkeywords}

\section{Introduction}
Write Intro

AWS - Charge for resources used.  Execution time and resource usage are related.  Same with CPU Time

%# Note: Use task metrics available through spark so that others can take advantage.
%Write jobs like beginners

\subsection{SparkMeasure and Spark 2.4.0}

Stage vs Task Stats and using task on local machine and stage on cluster for space concerns
%TODO: Mention that stage metrics peakExecutionMemory is max of task

\section{Spark Discussion RDD vs Dataset vs DataFrame}
%@Mariama

There are several differences between the three data formats that were used in this project.
Resilient Distributed Dataset (RDD) data is an immutable representation of distributed data.
RDD is best for unstructured data such as text and byte streams.
Datasets in the simplest terms are extensions of the Spark dataframe, and provide an object oriented interface.
DataFrames organize data into columns, and provide a domain-specific API to manipulate distributed data.
Data can be queried from both DataFrames and RDD through Spark SQL.\cite{b1}

Regarding the reported performance of the three data formats from other sources,
RDD offer a data format solution that is less expensive than the other data formats.
This is, however, at the sacrifice of data performance. For a more efficient data format, DataFrames and dataset provide
a means of attaining greater optimization.\cite{b3}
As specified in later sections, mapping and map partitions are optimizations that can be manually added to RDD
for efficiency.
Datasets in particular provides optimized queries through the Catalyst Query Optimizer (execution agnostic framework).
The above performance notes will be apparent in the visualizations and analysis of the graphs in the pages that follow.

\section{Data Discussion}
The data set used is the Global Surface Summary of the Day (GSOD) which contains data such as mean temperature, dew point, sea level pressure, mean wind speed, etc.
Each data file thoroughly covers approximately a 24-hours period.\cite{b7}
Cleanup was necessary because the data was not in an easily readable format such as CSV or JSON. Having the raw data and transforming this dats to CSV format ensured fair results for each of the 3 main dataformats used in the project, and enabled us to use built-in CSV parsing functionality.
Further data processing involves removing the headers for the data, as the headers were mostly incomplete.
The dataset in total is 2.6GB large with 121,467,184 records. The mere size of the data posed as a challenge on some of the machines that were used in the initial testing.


\section{Setup}
%TODO: Get Specs
The local machine is from running on a Mac.

\section{Map vs MapPartitions in RDD}
%TODO: We could describe partitioning in spark in more detail
%TODO: Rewrite sentence below
While writing Spark jobs, it is important to implement code that is efficient.
The Spark jobs written for the purpose of this paper are not necessarily the most efficient, especially since the definition of efficient is somewhat subjective.
This paper explores one definition of efficiency by examining the usage of available resources.
Thus, the question of using the map method versus the mapParititions method is a good starting to place since it will define how the remaining Spark jobs using RDDs get written.

One of the most popular efficiencies is the usage of the mapParititions method instead of the map method when transforming an RDD.
The map method will apply the specified transform to every element in the RDD.
The mapPartitions method operates on a partition of the RDD.
Simply put, a partition is a chunk of data in the RDD that can be operated on all at once without needing to shuffle the data.
So, a mapPartitions method may apply the same transform to every data element within its partition, thus acting like the map method would.
Keep in mind that mapPartitions may be used for more complicated operations utilizing all of the data elements within a partition.

While there are no traditional publications discussing the pros and cons of mapPartitions vs map, the general consensus on popular internet forums is that mapPartitions is generally faster because you do not have to set up a function for every element, just for the partition.
Since this section is not directly related to the purpose of this paper, a full written analysis will not be performed; however, this section provides information that fits into the larger purpose of this paper.

\subsection{Running on Local Machine}

When running on a local machine, the resources that can be given to a Spark job are limited.
Figure \ref{fig:localjobs} provides information on the resource settings for local machine runs.
This figure will be referenced in the other sections as well.
In order to ensure that the job would not run out of resources, a subset of the NOAA GSOD dataset was selected.
This subset of the NOAA GSOD data was everything from 2018.
This dataset consisted of 3443046 records totalling $\approx$ 74 megabytes.

\begin{figure}
    \includegraphics[width=\linewidth]{presentation_table.PNG}
    \caption{List of Job Numbers and their respective resource settings for local machine runs.}
    \label{fig:localjobs}
\end{figure}

\subsubsection{Execution Time Comparison}
The execution time of the Spark job can be found by subtracting the minimum value of the launchTime stat from the maximum of the finishTime stat.

\begin{figure}
    \includegraphics[width=\linewidth]{../python_scripts/images/mapVsMapPartitionsAllExecutionTime.png}
    \caption{Comparison of Map job vs MapPartitions job vs Basic RDD job}
    \label{fig:mapVMapPartitionsAllExecTime}
\end{figure}

The focus of this paper is on the importance of using benchmarks other than execution time for Spark analytics and Figure \ref{fig:mapVMapPartitionsAllExecTime} exemplifies this motivation by showing that:
\begin{itemize}
\item Even the exact same Spark analytic, such as the Basic RDD job and the MapPartitions job, may not consistently have the same execution time.
\item There can be anomalies in the execution time, which is likely due to the interference of other proccesses running on the same machine.
\end{itemize}

Additionally, Figure \ref{fig:mapVMapPartitionsAllExecTime} also shows that there is, as expected, a relationship between the number of threads running the job and the execution time.
For instance, doubling the threads from 1 to 2 cut the execution time in half.
Tripling the threads from 1 to 3 reduced the execution time to about a third of the execution time of 1 thread.
Increasing the number of threads from 1 to 4 also resulted in the run time decreasing to about 25\% of the original run time.

This confirms that increasing the parallelization of the job by increasing the number of threads will decrease the run time;
however, this does not confirm the commonly held belief that the mapPartitions method will execute faster than map method.
This transitions to the next question analyzed of whether there is an advantage when it comes to the peak execution memory.

\subsubsection{Peak Execution Memory Comparison}
\begin{figure}
    \includegraphics[width=\linewidth]{../python_scripts/images/mapVsMapPartitionsMaxPeakExecutionMemory.png}
    \caption{A boat.}
    \label{fig:mapVMapPartitionsMaxPeakExecutionMemory}
\end{figure}

%TODO: Should probably explain why max peak execution memory is the stat we focus on.
%TODO: Maybe dig deeper into this stat.  Why is it the same for all jobs.
Figure \ref{fig:mapVMapPartitionsMaxPeakExecutionMemory} shows that the peak execution memory is the same for both map and mapPartitions.
Since this is the case, the remaining Spark jobs working with RDDs will utilize mapPartitions instead of map.

%TODO: The comparison of the peakExecutionMemory vs BytesWritten for this job was interesting. Can use as space filler.
%-We may also want to look at some other memory metrics available

%TODO: Decide if we want to do the following CPU Time Comparison
%\subsubsection{CPU Time Comparison}
%\begin{figure}
%    \includegraphics[width=\linewidth]{../python_scripts/images/mapVsMapPartitionsAllCpuTime.png}
%    \caption{A boat.}
%    \label{fig:boat1}
%\end{figure}

\subsection{Running on Distributed Cluster}
The cluster these Spark jobs were run on was not very large and other jobs were running at the same time.
This is not a problem for our case since this paper is exploring the idea of resource benchmarking and once Spark was given resources, they were not taken away.
Figure \ref{fig:clusterjobs} provides information on the resource settings for the cluster runs.
This figure will be referenced in the other sections as well.
These jobs ran over all of the NOAA GSOD dataset.
This dataset consisted of 121467184 records totalling $\approx$ 2.6 gigabytes.

\begin{figure}
    \includegraphics[width=\linewidth]{cluster_run_settings.PNG}
    \caption{List of Job Numbers and their respective resource settings for cluster runs.}
    \label{fig:clusterjobs}
\end{figure}

Note that the jobs were not able to be run using only one or two executors due to cluster limitations on the maximum size of a container given to executors.
Similarly, the cluster could not support a higher number of

\subsubsection{Execution Time Comparison}

\begin{figure}
    \includegraphics[width=\linewidth]{../python_scripts/images/mapVsMapPartitionsAllExecutionTimeCluster.png}
    \caption{Comparison of the execution time of a Map job vs MapPartitions job vs Basic RDD job runs on the cluster}
    \label{fig:mapVMapPartitionsAllExecTimeCluster}
\end{figure}

Again, Figure \ref{fig:mapVMapPartitionsAllExecTimeCluster} exhibits how benchmarking Spark jobs based only off of execution time does not give the entire picture.
The Basic RDD and MapPartitions job are the exact same and yet, Figure \ref{fig:mapVMapPartitionsAllExecTimeCluster} shows the run times are not consistent.


\subsubsection{Peak Execution Memory Comparison}
\begin{figure}
    \includegraphics[width=\linewidth]{../python_scripts/images/mapVsMapPartitionsMaxPeakExecutionMemoryCluster.png}
    \caption{Comparison of maximum peak execution memory of a Map job vs MapPartitions job runs on the cluster.}
    \label{fig:mapVMapPartitionsMaxPeakExecutionMemoryCluster}
\end{figure}

By examining Figure \ref{fig:mapVMapPartitionsMaxPeakExecutionMemoryCluster}, it can be seen that in $3/4$ of the runs the mapPartitions method had a lower amount of peak execution memory.
Also note that the peak execution memory metric for the cluster runs is significantly higher than that of the local machine runs.
This makes sense because the size of the dataset was significantly larger.

Since it has been established above that the peak execution memory during a job using mapPartitions will be equal to the job using the map method on a local machine and will be lower the majority of runs in the cluster, the remaining RDD jobs are written using mapPartitions.

%Might be worthwhile to create another job that would call a toList on the partition iterator.

\section{Basic RDD vs Dataset vs Dataframe}\label{basicjobs}
The equivalent of the Hello World program for any big data analytics platform, including Spark, is Word Count.
The basic premise of the Word Count program is to count all of the unique words in a document.
The NOAA GSOD data does not easily support a Word Count program, so instead, a similar program was created to start the analysis of the peak execution memory for RDDs, DataFrames and Datasets.
In this job, a key was created by combining the station number and the nearest multiple of 10 of the average temperature that does not go over the average temperate (Ex 98 $\rightarrow$ 90).
Each unique key is the counted.
The output of the program is a count of all of the unique keys as this will force the program to run in its entirety to ensure equal comparison.

\subsection{Running on Local Machine}

The description of the resource settings for the runs on the local machine is the same as above and can be seen in Figure \ref{fig:localjobs}.


\subsubsection{Peak Execution Memory Comparison}
\begin{figure}
    \includegraphics[width=\linewidth]{../python_scripts/images/basicJobMaxPeakExecutionMemory.png}
    \caption{Comparison of maximum peak execution memory of the basic job runs on a local machine.}
    \label{fig:basicJobMaxPeakExecutionMemory}
\end{figure}

As can be seen in Figure \ref{fig:basicJobMaxPeakExecutionMemory}, the maximum peak execution memory for the DataFrame and Dataset implementations are the same.
This is expected since they are utilizing the same optimizations behind the scenes.
Additionally, Figure \ref{fig:basicJobMaxPeakExecutionMemory} shows a step like behavior for the peak execution memory for these two jobs based on the thread count.
The first four jobs are run using one thread, and thus, have a higher peak execution memory.
Upon examination of the job using the Spark UI, it can be seen that one of the optimizations of the Dataframes and Datasets is that it will repartition the data.
This will be discussed in further detail in Section \ref{section:partition}, but simply put, this means that more data is being put into a partition, so more memory is needed to deal with a single partition.
So, the next four jobs are run using two threads, so more partitions are created compared to running the job with one thread.
Thus, each partition is smaller in the two-threaded job than in the one threaded job.
Similarly, this also explains the step caused in the next 8 jobs when three and four threads are used.
The current working theory as to why there is not a step between the 3-threaded job and the 4-threaded job is because partitions are created based on the number of cores being used.
The local job was run on a machine with a quad-core processor, so one of the processors may have been assigned to the driver.

This RDD implementation also has a significantly lower peak execution memory, sitting at $\approx$ 4000 bytes.
The peak execution memory is so low, comparatively, for the RDD implementation because it does not have any optimization going on behind the scenes.
For each partition of data that the job has read in, it processes that partition individually during the count stage.
Thus, the peak execution memory remains the same.

This lead to the question of how does forcing the RDD to repartition affect the peak execution memory.
This will be explored in the next section.

\subsubsection{Execution Time Comparison}
As we can see in above, RDDs will use a lower amount of execution memory.
\begin{figure}
    \includegraphics[width=\linewidth]{../python_scripts/images/basicJobAllExecutionTime.png}
    \caption{Comparison of execution time of the basic job runs on a local machine.}
    \label{fig:basicJobAllExecutionTime}
\end{figure}

However, the graph in Figure \ref{fig:basicJobAllExecutionTime} shows that RDDs are also slower in the case of these jobs.
This makes sense because the overhead of preparing an executor to work with a partition is not insignificant.
So since the number of partitions that an RDD needs to work with is higher, it will take longer due this overhead.
Note that there were about 12000 partitions in the RDD during the count stage, while the DataFrame and Datasets were only working with at most 500 partitions.

Interestingly enough, the gap between the run times of the RDD vs DataFrames/Datasets closed as more threads were used.
This may indicate that at some point, having enough cores to run the job against removes the speed benefit of DataFrames and Datasets; however exploring this is out of scope.

\subsection{Running on Distributed Cluster}
Unfortunately, due to the clusters limited resources, the DataFrame and Dataset implementations of this basic count were unable to successfully complete with the configuration provided.
This does implies that their peak execution memory would be too high since the message displayed as to why the DataFrame and Datasets jobs died were because they used all of the resources and needed more.

Note that these jobs used the same resources settings as shown in Figure \ref{fig:clusterjobs}.

\subsubsection{Peak Execution Memory Comparison}
\begin{figure}
    \includegraphics[width=\linewidth]{../python_scripts/images/basicJobsMaxPeakExecutionMemoryCluster.png}
    \caption{Comparison of maximum peak execution memory of the basic job runs on a cluster.}
    \label{fig:basicJobsMaxPeakExecutionMemoryCluster}
\end{figure}

Figure \ref{fig:basicJobsMaxPeakExecutionMemoryCluster} shows that again, the maximum peak execution memory used is constant regardless of the number of executors or core per executor.
This makes sense because for this job with RDDs, the same partitions are going to be processed the same way, so the peak execution memory shouldn't flux much, if at all.

\subsubsection{Execution Time Comparison}
\begin{figure}
    \includegraphics[width=\linewidth]{../python_scripts/images/basicJobsAllExecutionTimeCluster.png}
    \caption{Comparison of execution time of the basic job runs on a cluster.}
    \label{fig:basicJobsAllExecutionTimeCluster}
\end{figure}

Figure \ref{fig:basicJobsAllExecutionTimeCluster} shows that the in both the 3 and 4 executor cases, having double the cores per executor significantly speeds up the execution time.

\section{Paritions RDD vs Dataset vs Dataframe}\label{section:partition}
%Why are partitions useful
The partition jobs are basically copies of the "Basic" jobs discussed in Section \ref{basicjobs}; however, the coalesce method was used on the raw input.
The coalesce method was used instead of the repartition method because repartition will force a shuffle.
The coalesce method did not force a shuffle and will instead just redistribute the old partitions to the specified number of new partitions.
Coalesce was the chosen method because even in the smaller job running on a local machine, over 12000 partitions were going to be created, so reducing the number of partitions should not require a shuffle to redistribute the data.
%TODO: Verify the following sentence.
Furthermore, coalesce was called on the raw input because that is guaranteed to be the one part that's shared between all of the jobs in this case;
however, Datasets and DataFrames have their own internal optimizations that may move that coalesce.

\subsection{Running on Local Machine}
\begin{figure}
    \includegraphics[width=\linewidth]{images/partition_local_job.png}
    \caption{List of Job Numbers and their respective resource settings for local runs of partition jobs.}
    \label{fig:partition_local_job}
\end{figure}

Figure \ref{fig:partition_local_job} shows the resource configurations of the cluster jobs for the partition jobs being run on a local machine.

\subsubsection{Resource Comparison for different partitions}
\begin{figure}
    \includegraphics[width=\linewidth]{../python_scripts/images/partitionJobMaxPeakExecutionMemory.png}
    \caption{Comparison of maximum peak execution memory of the partition job runs on a local machine.}
    \label{fig:partitionJobMaxPeakExecutionMemory}
\end{figure}

Like the previous job examples, we can see that again in all cases the RDD will have a lower peak execution memory, even when the partitions are the same.
Figure \ref{fig:partitionJobMaxPeakExecutionMemory} makes some of the PartitionRDD jobs look like they did not run and have a peak execution memory of 0.
The RDD jobs did run and the max peak execution memory for the PartitionRDD has a min of $\approx$ 16000 bytes.

Figure \ref{fig:partitionJobMaxPeakExecutionMemory} also shows that for every underlying datatype, the number of partitions has a large effect of the peak execution memory.
For all of them, the highest peak execution memory occurred when the data was coalesced to one partition, which makes sense because then all of the data is being worked with at once.
Interestingly, the difference in peak execution memory when using 100 vs 500 vs 1000 partitions was almost negligible.
This would indicate that there is not much benefit in using many more than 100 partitions for these jobs in terms of trying to lower the peak execution memory.

\subsubsection{Execution Time Comparison}
\begin{figure}
    \includegraphics[width=\linewidth]{../python_scripts/images/partitionJobAllExecutionTime.png}
    \caption{Comparison of execution time of the partition job runs on a local machine.}
    \label{fig:partitionJobAllExecutionTime}
\end{figure}

Like the previous execution time graphs, Figure \ref{fig:partitionJobAllExecutionTime} shows why execution time is a bad benchmark.
For example, most of the Dataset and DataFrame backed jobs have very similar run times; however, there is the random spike in run time at job 3.

\subsection{Running on Distributed Cluster}

Unfortunately, the PartitionRDD job was the only job that completed when running on the cluster.
The PartitionDataset and PartitionDataFrame jobs were not able to complete because they ran out of resources.

Figure \ref{fig:partition_local_job} shows the resource configurations of the cluster jobs for the partition jobs being run on a local machine.

\begin{figure}
    \includegraphics[width=\linewidth]{images/partition_cluster_job.png}
    \caption{List of Job Numbers and their respective resource settings for runs on a cluster.}
    \label{fig:cluster_local_job}
\end{figure}

\subsubsection{Resource Comparison for different partitions}
\begin{figure}
    \includegraphics[width=\linewidth]{../python_scripts/images/partitionJobsMaxPeakExecutionMemoryCluster.png}
    \caption{Comparison of peak execution memory of the partition job runs on a cluster.}
    \label{fig:partitionJobsMaxPeakExecutionMemoryCluster}
\end{figure}

As can be seen in Figure \ref{fig:partitionJobsMaxPeakExecutionMemoryCluster}, job numbers 3 and 7 of the PartitionRDD job also failed due to lack of resources.
An interesting thing that can be seen in this graph is that the maximum peak execution memory actually increases as the number of partitions increase.
This is surprising since the behavior was the opposite on the local run.
In the local runs of this job, the peak execution memory decreased as the number of partitions increased.
This is possibly because of the coalesce behavior.
This job was trying to get the $\approx$ 400000 partitions read in condensed into a much smaller number of partitions.
We can also compare this peak execution memory to that in Figure \ref{fig:basicJobsMaxPeakExecutionMemoryCluster} for the MapPartitions job because that job just worked with the original partitions instead of trying to coalesce.
The peak execution memory of the MapPartitions job is smaller than that of the PartitionRDD job, which is likely due to the fact that the MapPartitions job is working with smaller clusters.

Additionally, it is important to note that the difference in the peak execution memory for the jobs running with only one core per executor and those running with two cores per executors is almost negligible.

\subsubsection{Execution Time Comparison}
Since the PartitionDataset and PartitionDataFrame jobs did not complete, this section will not be analyzed since no comparison can be made.

\section{Cache RDD vs Dataset vs Dataframe}
The caching jobs are similar

%TODO: Reference KMeans jobs here
Talk about the job. Basically a grouping + count + count.  Discuss where the caching happens.

\subsection{Discuss Issues with Caching}
It looks like data had to spill to disk during the caching. We need to discuss this and other metrics that could be used
to explain this behavior.

\subsection{Resource Comparison for different partitions}
Create a line graph with 3 options to compare RDD vs dataframe vs dataset and the jobs per peakExecutionMemory.  Need to explore other metrics

\subsection{Execution Time Comparison}
Create a line graph with 3 options to compare RDD vs dataframe vs dataset and the jobs per peakExecutionMemory for execution time. Need to explore other metrics

Should we explore other caching levels?
%https://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.rdd.RDD@persist(newLevel:org.apache.spark.storage.StorageLevel):RDD.this.type

And we can discuss KMeans since those jobs were run.







\subsection{NOAA Dataset}\label{NOAA}
The data set used is the Global Surface Summary of the Day (GSOD) which contains data such as mean temperature, dew point, sea level pressure, mean wind speed, etc.
Each data file thoroughly covers approximately a 24-hours period.
Cleanup was necessary because the data was not in an easily readable format such as CSV or JSON. Having the data as a raw comparison a bit more “fair” since we could now use built-in CSV parsing functionality.
Headers for the data were not complete,therefore they were removed. In order to use the data for analysis, the data was transformed into CSV.


\begin{thebibliography}{00}
\bibitem{b1} J. Damji, ``A Tale of Three Apache Spark APIs: RDDs vs DataFrames and Datasets
When to use them and why'',2016. [Online] Available: https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html.[Accessed: 16- Dec- 2016].
\bibitem{b2} Dataflair Team, ``Apache Spark RDD vs DataFrame vs DataSet'', 2018. [Online] Availiable: https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/. [Accessed: 16- Dec- 2018].
\bibitem{b3} Dataflair Team, ``Spark Dataset Tutorial – Introduction to Apache Spark Dataset'', 2018. [Online] Availiable: https://data-flair.training/blogs/apache-spark-dataset-tutorial/. [Accessed: 16- Dec- 2018].
\bibitem{b4} A. Bonsanto, ``Apache Spark: map vs mapPartitions?'',2014. [Online] Availiable: https://stackoverflow.com/questions/21185092/apache-spark-map-vs-mappartitions. [Accessed: 16- Dec- 2018].
\bibitem{b5} L. Martin, ``Why spark's mapPartitions transformation is 
faster than map (calls your function once/partition, not once/element)'',2016. [Online] Availiable: https://martin.atlassian.net/wiki/spaces/lestermartin/blog/2016/05
/19/67043332/why+spark++mapPartitions+transformation+is+faster+than
+map+calls+your+function+once+partition+not+once+element [Accessed: 16- Dec- 2018].
\bibitem{b6} K. Kumar, ``What is the difference between a map and mapPartitions in Spark?'',2017. [Online] Availiable: https://www.quora.com/What-is-the-difference-between-a-map-and-mapPartitions-in-Spark. [Accessed: 16- Dec- 2018].
\bibitem{b6} Kaggle, NOAA, ``NOAA Global Surface Summary of the Day'', 2018. [Online] Avaliable: 
https://www.kaggle.com/noaa/noaa-global-surface-summary-of-the-day/home. [Accessed: 19- Dec- 2018].
\end{thebibliography}
\vspace{12pt}

\end{document}
